{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "BOARD_XSIZE=7\n",
    "BOARD_YSIZE=6\n",
    "DIMS=(BOARD_YSIZE,BOARD_XSIZE)\n",
    "\n",
    "EPISODES_PER_AGENT = 100\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = network.Actor(BOARD_XSIZE, BOARD_YSIZE).to(device)\n",
    "critic = network.Critic(BOARD_XSIZE, BOARD_YSIZE).to(device)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=network.ACTOR_LR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent_pool:list[player.Player] = [\n",
    "    player.RandomPlayer(env.PLAYER2),\n",
    "    player.MinimaxPlayer(env.PLAYER2, 2, 0.1),\n",
    "    player.MinimaxPlayer(env.PLAYER2, 2, 0.3),\n",
    "    player.MinimaxPlayer(env.PLAYER2, 2, 0.5),\n",
    "]\n",
    "\n",
    "rewards_vs: dict[str, list[float]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(actor:player.ActorPlayer, opponent: player.Player, actor_turn:bool) -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[env.Reward],\n",
    "    list[env.Advantage],\n",
    "    list[env.Reward],\n",
    "]:\n",
    "    e = env.Env(DIMS)\n",
    "\n",
    "    s_t:list[env.Observation] = []\n",
    "    a_t:list[env.Action] = []\n",
    "    r_t:list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        if actor_turn:\n",
    "            obs, chosen_action, reward = actor.play(e)\n",
    "            s_t += [obs]\n",
    "            a_t += [chosen_action]\n",
    "            r_t += [reward]\n",
    "        else:\n",
    "            opponent.play(e)\n",
    "\n",
    "        # flip turn\n",
    "        actor_turn = not actor_turn\n",
    "\n",
    "    # compute advantage and value\n",
    "    d_t = network.compute_advantage(actor.critic, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    return s_t, a_t, r_t, d_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interrupt this cell when you're done training\n",
    "\n",
    "for _ in range(TRAIN_EPOCHS):\n",
    "    s_batch:list[env.Observation] = []\n",
    "    a_batch:list[env.Action] = []\n",
    "    p_batch:list[np.ndarray] = []\n",
    "    d_batch:list[env.Advantage] = []\n",
    "    v_batch:list[env.Value] = []\n",
    "    \n",
    "    # create actor player\n",
    "    actor_player = player.ActorPlayer(actor, critic, step, env.PLAYER1)\n",
    "    \n",
    "    for _ in range(EPISODES_PER_AGENT):\n",
    "        # pick a random opponent\n",
    "        opponent_player = opponent_pool[np.random.randint(len(opponent_pool))]\n",
    "\n",
    "        # whether we or our opponent goes first\n",
    "        go_first = np.random.randint(2) == 0\n",
    "\n",
    "        # play the game\n",
    "        s_t, a_t, r_t, d_t, v_t = play(actor_player,opponent_player, go_first)\n",
    "\n",
    "        # now update the minibatch\n",
    "        s_batch += s_t\n",
    "        a_batch += a_t\n",
    "        d_batch += d_t\n",
    "        v_batch += v_t\n",
    "\n",
    "        # statistics\n",
    "        opp_name = opponent_player.name()\n",
    "        if opp_name in rewards_vs:\n",
    "            rewards_vs[opp_name].append(float(v_t[-1]))\n",
    "        else:\n",
    "            rewards_vs[opp_name] = [float(v_t[-1])]\n",
    "\n",
    "    actor_losses, critic_losses = network.train_ppo(\n",
    "        actor,\n",
    "        critic,\n",
    "        actor_optimizer,\n",
    "        critic_optimizer,\n",
    "        s_batch,\n",
    "        a_batch,\n",
    "        d_batch,\n",
    "        v_batch\n",
    "    )\n",
    "\n",
    "    for actor_loss, critic_loss in zip(actor_losses, critic_losses):\n",
    "        writer.add_scalar('actor_loss', actor_loss, step)\n",
    "        writer.add_scalar('critic_loss', critic_loss, step)\n",
    "\n",
    "        if step % SUMMARY_STATS_INTERVAL == 0:\n",
    "            for opponent_name, rewards in rewards_vs.items():\n",
    "                if len(rewards) > 50:\n",
    "                    avg_reward = np.array(rewards).mean()\n",
    "                    writer.add_scalar(f'reward_against_{opponent_name}', avg_reward, step)\n",
    "                    rewards_vs[opponent_name] = []\n",
    "\n",
    "        if step % MODEL_SAVE_INTERVAL == 0:\n",
    "            # Save the neural net parameters to disk.\n",
    "            torch.save(actor.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_actor.ckpt\")\n",
    "            torch.save(critic.state_dict(), f\"{SUMMARY_DIR}/nn_model_ep_{step}_critic.ckpt\")\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load('./summary/nn_model_ep_500_actor.ckpt'))\n",
    "#critic.load_state_dict(torch.load('./summary/nn_model_ep_1500_critic.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = env.Env(DIMS)\n",
    "\n",
    "e.step(env.Action(1), env.PLAYER1)\n",
    "e.step(env.Action(1), env.PLAYER1)\n",
    "e.step(env.Action(1), env.PLAYER1)\n",
    "\n",
    "e.step(env.Action(5), env.PLAYER2)\n",
    "e.step(env.Action(5), env.PLAYER2)\n",
    "e.step(env.Action(5), env.PLAYER2)\n",
    "\n",
    "\n",
    "\n",
    "o = e.observe(1)\n",
    "print(e.legal_mask())\n",
    "env.print_obs(o)\n",
    "print('0 1 2 3 4 5 6 7')\n",
    "print(actor.forward(network.obs_to_tensor(o, device))[0])\n",
    "print(critic.forward(network.obs_to_tensor(o, device))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to observe some games from the network\n",
    "\n",
    "s_tensor = network.obs_batch_to_tensor(s_batch, device)\n",
    "critic_guesses = critic.forward(s_tensor).to(cpu).detach().numpy()\n",
    "actor_guesses = actor.forward(s_tensor).to(cpu).detach().numpy()\n",
    "for v, obs, critic_guess, actor_guess in zip(v_batch, s_batch, critic_guesses, actor_guesses):\n",
    "    print(\"real_value\", v)\n",
    "    print(\"pred_value\", float(critic_guess))\n",
    "    print(\"actor_probs\", np.array(actor_guess))\n",
    "    env.print_obs(obs)\n",
    "    print('0 1 2 3 4 5 6 7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you try!\n",
    "e = env.Env(DIMS)\n",
    "\n",
    "actor_turn = True\n",
    "opponent = player.ActorPlayer(actor, critic, step, env.PLAYER2)\n",
    "human_player = player.HumanPlayer(env.PLAYER1)\n",
    "# minmax player with depth 3 and no randomness\n",
    "\n",
    "while not e.game_over():\n",
    "    if actor_turn:\n",
    "        obs, chosen_action, reward = human_player.play(e)\n",
    "    else:\n",
    "        opponent.play(e)\n",
    "    # flip turn\n",
    "    actor_turn = not actor_turn\n",
    "\n",
    "if e.winner() == env.PLAYER1:\n",
    "    print(\"You win!\")\n",
    "elif e.winner() == env.PLAYER2:\n",
    "    print(\"You lose!\")\n",
    "else:\n",
    "    print(\"Draw!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
